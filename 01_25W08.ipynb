{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List \n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local imports from tools package\n",
    "from tools import brikasutils as bu\n",
    "from tools import shared_utils as utils\n",
    "from tools.shared_utils import systemMsg, userMsg, assistantMsg\n",
    "from tools import survey\n",
    "from tools import persona\n",
    "\n",
    "# Reload modules\n",
    "importlib.reload(bu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(survey)\n",
    "importlib.reload(persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_NAMEID = \"airidas_finalboss_1\"\n",
    "\n",
    "import json\n",
    "with open(f\"embeddings/{EMBEDDING_NAMEID}_embeddings.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data[\"chunks\"]\n",
    "    embeddings = data[\"embeddings\"]\n",
    "\n",
    "try:\n",
    "    with open(f\"embeddings/{EMBEDDING_NAMEID}_info.json\", \"r\") as f:\n",
    "        AUTO_INFO = json.load(f)\n",
    "        try:\n",
    "            EMBED_MODEL = AUTO_INFO[\"model\"]\n",
    "            chunk_size = AUTO_INFO[\"CHUNK_SIZE\"]\n",
    "        except KeyError:\n",
    "            print(\"WARNING: Info text does not contain model information\")\n",
    "except:\n",
    "    print(\"WARNING: No Info file found. Make sure embedding model is matching.\")\n",
    "\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIAVAL_METHOD = \"dynamic\" #static/dynamic/hybrid\n",
    "PROMPT_METHOD = \"IMPERSONATE\" #ARE/IMPERSONATE\n",
    "\n",
    "SUBJECT = \"Elias\"\n",
    "\n",
    "# Load Embeddings From File (optional)\n",
    "import json\n",
    "# with open(f\"data/4_chunks/{EMBEDDING_ID}-{CHUNKS_COUNT_IN_CTX}_{VERSION_ID}-dynamic_embeddings.json\", \"r\") as f:\n",
    "with open(f\"data/4_chunks/{CHECKPOINT}-{RETRIAVAL_METHOD}_embeddings.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    chunks = data[\"chunks\"]\n",
    "    embeddings = data[\"embeddings\"]\n",
    "print(f\"Chunks:{len(chunks)}, embeds:{len(embeddings)}\")\n",
    "\n",
    "# \"PERSONA_TEXT\": \"Favorite video games are Minecraft, Fortnite, and Call of Duty.\",\n",
    "# \"MED_MODULE\": \" \"\n",
    "# SURVEY_PROMPT = \"Determine how much {subject} aggree with the statement. Guestimate how {subject} would answer to the question\"\n",
    "TINY_MODULE = \"You are Elias, a 24 year old business and IT student from Copenhagen, where you now live in a dormatory.\"\n",
    "\n",
    "####################### You are {SUBJECT} vs you will impersonate {SUBJECT} #####################\n",
    "PREP_CHECKPOINT = f\"{CHECKPOINT}-{RETRIAVAL_METHOD}_{SUBJECT}-{PROMPT_METHOD}\"\n",
    "PREP_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Method A ############################################\n",
    "if PROMPT_METHOD == \"IMPERSONATE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "SYS_MSG = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n\\\\n**The persona, which you will be tasked to mimick is named '{SUBJECT}'.** \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\"\n",
    "}\n",
    "ASSIST_MSG = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": f\"Understood. I will answer from the point of view of the persona, {SUBJECT}, based on what I could the deduct from the text provided.\"\n",
    "}\n",
    "USER_MSG = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\"\n",
    "}\n",
    "\"\"\"\n",
    "########################################### Method B ###########################################\n",
    "elif PROMPT_METHOD == \"ARE\":\n",
    "    pre_prompt_template = \"\"\"\n",
    "SYS_MSG = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": f\"**{TINY_MODULE}**. You have shared your thoughts, feelings, and experiences through text messages with friedns. Answer the following questions honestly and naturally, as you would in everyday conversations. \\\\n\\\\n#Context \\\\n##Conversations between persona and friends:\"\n",
    "}\n",
    "ASSIST_MSG = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": f\"Understood. I am {SUBJECT}, and I will answer the survey to the best of my ability.\"\n",
    "}\n",
    "USER_MSG = {   \n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"The survey is about your {SURVEY}. You must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Your answer must only contain the chosen option, without any elaboration, nor introduction.\\n**From most to least related**\\\\n\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "exec(pre_prompt_template)\n",
    "\n",
    "print(f\"{SYS_MSG['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {\n",
    "    \"Which survey\": surv,\n",
    "    # \"Prompt method\": f\"You are {SUBJECT} vs you will impersonate {SUBJECT}\",\n",
    "    # \"Retrieval method\": \"Dynamic/static/hybrid\",\n",
    "    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    \"EMBEDDING_ID\": EMBEDDING_ID,\n",
    "    \"VERSION_ID\": VERSION_ID,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"EMBED_MODEL\": EMBED_MODEL,\n",
    "    \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    \"EMBEDDING_ID\": EMBEDDING_ID,\n",
    "    \"DYNAMIC_CHUNKS_COUNT\": len(dynamic_chunks_most_similar),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "SUBJECT = \"airidas\"\n",
    "RETRIAVAL_METHODS = [\"static\"]#, \"hybrid\", \"dynamic\", \n",
    "NUM_RUNS = 3\n",
    "# MODEL = \"llama3-70b\"\n",
    "# MODEL = \"llama3-8b\"\n",
    "MODEL = \"mixtral-8x22b\"\n",
    "# max_tokens = [7500]\n",
    "max_tokens = [0, 4000, 7500]\n",
    "\n",
    "survs = [survey.KanoSurvey(), survey.PersonalitySurvey()]\n",
    "for surv in survs:\n",
    "    if isinstance(surv, survey.KanoSurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 40\n",
    "        SURVEY_TYPE = \"KanoSurvey\",\n",
    "        WHICH_SURVEY = \"kano\"\n",
    "        RETRIEVAL_PROMPT = \"video game features\"\n",
    "        SURVEY = \"video game preferences\"\n",
    "        METHOD = \"a Kano survey\"\n",
    "    elif isinstance(surv, survey.PersonalitySurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 50\n",
    "        SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "        WHICH_SURVEY = \"pers\"\n",
    "        RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "        SURVEY = \"personality traits\"\n",
    "        METHOD = \"an OCEAN test\"\n",
    "\n",
    "    for max_token in max_tokens:\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "\n",
    "                # Dynamic Retrieval\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "\n",
    "                    ## Chunking and Cosine Similarity\n",
    "                    max_chunks_count = int((max_token / avg_chunk_token_count))\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "\n",
    "                    # Limit chunks by tokens\n",
    "                    if max_token == 0:\n",
    "                        chunks_most_similar = chunks_most_similar[:1]\n",
    "                    else:\n",
    "                        # Limit chunks by man token count\n",
    "                        cur_tc = 0 # current token count\n",
    "                        selected_chunks = []\n",
    "                        for chunk in chunks_most_similar:\n",
    "                            tk_in_chunk = utils.count_tokens(chunk)\n",
    "                            if cur_tc + tk_in_chunk >= max_token:\n",
    "                                break\n",
    "                            cur_tc += tk_in_chunk\n",
    "                            selected_chunks.append(chunk)\n",
    "                        chunks_most_similar = selected_chunks\n",
    "                    \n",
    "                    # Finalize chunks\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "\n",
    "                # Count total tokens in all chunks\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                # del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famous people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),     \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "\n",
    "            elif RETRIAVAL_METHOD == \"static\":\n",
    "                prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "                chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "                chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "                ## Chunking\n",
    "                if max_token == 0:\n",
    "                    chunks_most_similar = chunks_most_similar[:1]\n",
    "                else:\n",
    "                    # Limit chunks by man token count\n",
    "                    cur_tc = 0 # current token count\n",
    "                    selected_chunks = []\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tk_in_chunk = utils.count_tokens(chunk)\n",
    "                        if cur_tc + tk_in_chunk >= max_token:\n",
    "                            break\n",
    "                        cur_tc += tk_in_chunk\n",
    "                        selected_chunks.append(chunk)\n",
    "                    chunks_most_similar = selected_chunks\n",
    "                \n",
    "                # Count total tokens in all chunks\n",
    "                tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                # static_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(chunks_most_similar)}\")\n",
    "\n",
    "                del chunks_most_similar_embeddings # free memory\n",
    "                #Retrieve^\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famous people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),     \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: raise ValueError(f\"Unknown retrieval method: {RETRIAVAL_METHOD}\")\n",
    "            exec(prompt_template)\n",
    "             # Save prompts\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{RETRIAVAL_METHOD}-{max_token}_{MODEL}_V8\"\n",
    "            bu.quickJSON(final_prompts, f\"data/temp/{SIM_ID}_prompts.json\")\n",
    "            for num_run in range(NUM_RUNS):\n",
    "                instructions = {\n",
    "                    \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                    \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                    \"isLocal\": True,\n",
    "                    \"LIMIT\": None\n",
    "                }\n",
    "                settings = {\n",
    "                    \"model\": MODEL,\n",
    "                    \"timeout\": 300}\n",
    "                AUTO_INFO = {\n",
    "                    \"CHUNK_SIZE\": chunk_size,\n",
    "                    \"OVERLAP_SIZE\": overlap_size,\n",
    "                    \"CTX_limit\": max_token,\n",
    "                    \"chunk_count\": len(chunks_most_similar),\n",
    "                    \"EMBED_MODEL\": EMBED_MODEL,\n",
    "                    \"prompt method\": PROMPT_METHOD,\n",
    "                    \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "                    \"prompt_count\": PROMPT_COUNT,\n",
    "                    \"survey\": WHICH_SURVEY,\n",
    "                    \"SUBJECT\": SUBJECT,\n",
    "                    \"prompt_template\": prompt_template,\n",
    "                    # \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,\n",
    "                    **utils.describe_prompts(final_prompts)\n",
    "                }\n",
    "                bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/temp/runs/{SIM_ID}_{num_run}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V6 & Hybrid legacy versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYBRID\n",
    "\n",
    "#             elif RETRIAVAL_METHOD == \"hybrid\":\n",
    "#                 # Access global static chunks\n",
    "#                 hybrid_chunks_most_similar: List[List[str]] = []\n",
    "\n",
    "#                 # Retrieve chunks for RETRIEVAL_PROMPT\n",
    "#                 prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "#                 global_static_chunks_most_similar = [chunks[embedding[1]] for embedding in utils.find_most_similar(prompt_embedding, embeddings)]\n",
    "#                 # retrieval_prompt_chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "#                 # static_chunks_most_similar = [chunks[embedding[1]] for embedding in retrieval_prompt_chunks_most_similar_embeddings]\n",
    "#                 for chunk in global_static_chunks_most_similar:\n",
    "#                     p.append(chunk)\n",
    "\n",
    "#                 # Retrieve chunks for prompt\n",
    "#                 prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "#                 prompt_chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "#                 dynamic_chunks_most_similar = [chunks[embedding[1]] for embedding in prompt_chunks_most_similar_embeddings]\n",
    "\n",
    "#                 if max_token == 0:\n",
    "#                     p.append([static_chunks_most_similar[0], dynamic_chunks_most_similar[0]])  # Edge case: 1 chunk from each\n",
    "                \n",
    "#                 # Combine chunks while limiting by max_token\n",
    "#                 combined_chunks = []\n",
    "#                 cur_tc = 0  # current token count\n",
    "#                 for chunk in global_static_chunks_most_similar + dynamic_chunks_most_similar:\n",
    "#                     tk_in_chunk = utils.count_tokens(chunk)\n",
    "#                     if cur_tc + tk_in_chunk >= max_token:\n",
    "#                         break\n",
    "#                     cur_tc += tk_in_chunk\n",
    "#                     combined_chunks.append(chunk)\n",
    "\n",
    "#                 hybrid_chunks_most_similar.append(combined_chunks)\n",
    "#                 tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in combined_chunks)\n",
    "#                 print(f\"Tokens in average chunk group: {tokens_in_chunks/len(hybrid_chunks_most_similar)}\")\n",
    "#                 #Retrieve^\n",
    "#                 # for question in surv.questions:\n",
    "#                 #     p = [question]\n",
    "#                 # for chunk in chunks_most_similar[i]:\n",
    "#                 #     p.append(chunk)\n",
    "#                 #     i += 1\n",
    "#                 final_prompts = []\n",
    "#                 prompt_template = \"\"\"\n",
    "# p = []\n",
    "# for chunk in chunks_most_similar:\n",
    "#     p.append(chunk)\n",
    "\n",
    "#     p = [\n",
    "#         systemMsg(\"\\\\n\".join([\n",
    "#             f\"You are an expert actor, specializing in impersonation of non-famous people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n\",\n",
    "#             \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "#         ])),     \n",
    "#         assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "#         userMsg(\"\\\\n\".join([\n",
    "#             f\"Persona is questioned about their {SURVEY} in {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "#             question,\n",
    "#             \"\\\\nThe persona chooses:\"\n",
    "#         ]))]\n",
    "#     final_prompts.append(p)\n",
    "#                 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZES = [250]\n",
    "MODELS = [\"mixtral:8x22b-instruct-v0.1-q2_K\"]\n",
    "# OVERLAP_SIZES = [5]\n",
    "overlap_size = 5\n",
    "RETRIAVAL_METHODS = [\"dynamic\", \"hybrid\"]\n",
    "max_tokens = 10000\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    for MODEL in MODELS:\n",
    "        chunks = []\n",
    "        chunk_token_counts = []\n",
    "        for chat in et.selectedChats.values():\n",
    "            messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "            num_messages = len(messages)\n",
    "            for i in range(0, num_messages - chunk_size + 1, chunk_size - overlap_size):\n",
    "                chunk = messages[i:i + chunk_size]  # Extract chunk of messages\n",
    "                chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "                chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "                chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "        avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "        embeddings = []\n",
    "        progress, chunks_len = 0, len(chunks)\n",
    "        for chunk_text in chunks:\n",
    "            progress += 1\n",
    "            print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "            embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "            embeddings.append(embedding)\n",
    "        for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "            if RETRIAVAL_METHOD == \"dynamic\":\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = [] \n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "                    max_chunks_count = int((max_tokens / avg_chunk_token_count)-1)\n",
    "                    chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count]\n",
    "                    chunks_most_similar = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings]\n",
    "                    tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar)\n",
    "                print(end=\"\\n\")\n",
    "                tokens_in_chunks = 0\n",
    "                for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "                    for chunk in chunks_most_similar:\n",
    "                        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "                del chunks_most_similar_embeddings  # free memory\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),      \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            elif RETRIAVAL_METHOD == \"hybrid\":\n",
    "                prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=RETRIEVAL_PROMPT)[\"embedding\"]\n",
    "                max_chunks_count = int((max_tokens / avg_chunk_token_count))\n",
    "                chunks_most_similar_embeddings_static = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count // 2]\n",
    "                chunks_most_similar_static = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings_static]\n",
    "\n",
    "                dynamic_retrieval_prompts = list(surv.questions)\n",
    "                dynamic_chunks_most_similar: List[List[str]] = []\n",
    "                progress = 0\n",
    "                lenn = len(dynamic_retrieval_prompts)\n",
    "                for prompt in dynamic_retrieval_prompts:\n",
    "                    progress += 1\n",
    "                    print(f\"\\rPrompt {progress}/{lenn}\", end=\"\")\n",
    "                    prompt_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=prompt)[\"embedding\"]\n",
    "                    chunks_most_similar_embeddings_dynamic = utils.find_most_similar(prompt_embedding, embeddings)[:max_chunks_count // 2]\n",
    "                    chunks_most_similar_dynamic = [chunks[embedding[1]] for embedding in chunks_most_similar_embeddings_dynamic]\n",
    "                    dynamic_chunks_most_similar.append(chunks_most_similar_dynamic)\n",
    "                print(end=\"\\n\")\n",
    "\n",
    "                chunks_most_similar = chunks_most_similar_static + [chunk for sublist in dynamic_chunks_most_similar for chunk in sublist]\n",
    "                tokens_in_chunks = sum(utils.count_tokens(chunk) for chunk in chunks_most_similar)\n",
    "                del chunks_most_similar_embeddings_static, chunks_most_similar_embeddings_dynamic  # free memory\n",
    "\n",
    "                print(f\"Tokens in average chunk group: {tokens_in_chunks / len(chunks_most_similar)}\")\n",
    "                final_prompts = []\n",
    "                prompt_template = \"\"\"\n",
    "for question, chunks_most_similar_dynamic in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"You are an expert actor, specializing in impersonation of non-famouns people. You will be presented to the subject through explicit datapoints of their digital footprint. In addition, you will deduct their implicit {SURVEY} by shadowing chats between the subject and friends. You will be asked to fully immerse yourself in the role, and answer questions from the point of view of the persona. \\\\n#Context \\\\n##Chat conversations between the subject and their friends:\\\\n**From most to least related**\\\\n\",\n",
    "            \"\\\\n\\\\nNEW CONVERSATION RELATED TO THE SURVEY OVERALL:\\\\n\".join(chunks_most_similar_static),\n",
    "            \"\\\\n\\\\nNEW CONVERSATION RELATED TO THE PARTICULAR QUESTION:\\\\n\".join(chunks_most_similar_dynamic)\n",
    "        ])),      \n",
    "        assistantMsg(\"Understood. I will answer from the point of view of the persona, based on what I could the deduct from the text provided.\"),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"Persona is questioned about their {SURVEY} in an {METHOD}. The persona must choose an appropriate answer to the question below with one of these five given options: {', '.join(surv.POSSIBLE_ANSWERS)}. Persona's answer must only contain the chosen option, without any elaboration, nor introduction.\\\\n\\\\n**Your question is:**\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "                \"\"\"\n",
    "            else: print(\"neither hybrid, nor dynamic\")\n",
    "            exec(prompt_template)\n",
    "            prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "            SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{RETRIAVAL_METHOD}-{chunk_size}-{str(overlap_size).zfill(2)}-{str(max_chunks_count).zfill(2)}_{MODEL}_V6\"\n",
    "            bu.quickJSON(final_prompts, f\"data/5_monster_prep/{SIM_ID}_prompts.json\")\n",
    "            instructions = {\n",
    "                \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                \"isLocal\": True,\n",
    "                \"LIMIT\": None\n",
    "            }\n",
    "            settings = {\n",
    "                \"model\": MODEL,\n",
    "                \"timeout\": 300}\n",
    "            AUTO_INFO = {\n",
    "                \"CHUNK_SIZE\": chunk_size,\n",
    "                \"OVERLAP_SIZE\": overlap_size,\n",
    "                \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,#chunks_count_in_ctx,\n",
    "                \"CTX_limit\": max_tokens,\n",
    "                \"tokens_in_chunks\": tokens_in_chunks,\n",
    "                \"model\": EMBED_MODEL,\n",
    "                \"prompt method\": PROMPT_METHOD,\n",
    "                \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                \"retrieval prompt\": RETRIEVAL_PROMPT,\n",
    "                \"prompt_count\": PROMPT_COUNT,\n",
    "                \"survey\": WHICH_SURVEY,\n",
    "                \"subject\": SUBJECT,\n",
    "                \"prompt_template\": prompt_template,\n",
    "                **prompt_info,\n",
    "                **utils.describe_prompts([])\n",
    "            }\n",
    "            bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch-schema/{SIM_ID}_schema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "for question, chunks_most_similar in zip(surv.questions, dynamic_chunks_most_similar):\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"{SYS_MSG['content']}\",\n",
    "            \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),      \n",
    "        assistantMsg(ASSIST_MSG['content']),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"{USER_MSG['content']}\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "\"\"\"    \n",
    "exec(prompt_template)\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\"\\\\n\".join([\n",
    "            f\"{SYS_MSG['content']}\",\n",
    "            \"\\\\nNEW CONVERSATION:\\\\n\".join(chunks_most_similar)\n",
    "        ])),  \n",
    "        assistantMsg(ASSIST_MSG['content']),\n",
    "        userMsg(\"\\\\n\".join([\n",
    "            f\"{USER_MSG['content']}\\\\n\\\\n**Your question is:**\\\\n\\\\n\",\n",
    "            question,\n",
    "            \"\\\\nThe persona chooses:\"\n",
    "        ]))]\n",
    "    final_prompts.append(p)\n",
    "\"\"\"\n",
    "exec(prompt_template)\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts)\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{PREP_CHECKPOINT}_prompts.json\")\n",
    "print(f\"{len(final_prompts)}\")#,{final_prompts[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base (no persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your {SURVEY}.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        )]\n",
    "    final_prompts.append(p)\n",
    "\"\"\"\n",
    "exec(prompt_template)\n",
    "prompt_info = utils.describe_prompts_and_print(final_prompts) # Vanity print\n",
    "bu.quickJSON(final_prompts, f\"data/5_prep/{WHICH_SURVEY}_base_prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3\"\n",
    "\n",
    "instructions = {\n",
    "    \"prompt_file\": f\"batch/prompts/{WHICH_SURVEY}_base_prompt.json\",\n",
    "    \"survey_type\": f\"{SURVEY_TYPE}\",\n",
    "    \"isLocal\": True,\n",
    "    \"LIMIT\": None\n",
    "}\n",
    "settings = {\n",
    "    \"model\": MODEL,\n",
    "    \"timeout\": 300\n",
    "}\n",
    "AUTO_INFO = {\n",
    "    \"survey\": WHICH_SURVEY,\n",
    "    \"prompt_template\": prompt_template,\n",
    "    **utils.describe_prompts([])\n",
    "    }\n",
    "bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_prep/{WHICH_SURVEY}_base_batch-schema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
