{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from typing import List \n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local imports from tools package\n",
    "from tools import brikasutils as bu\n",
    "from tools import shared_utils as utils\n",
    "from tools.shared_utils import systemMsg, userMsg, assistantMsg\n",
    "from tools import survey\n",
    "from tools import persona\n",
    "\n",
    "# Reload modules\n",
    "importlib.reload(bu)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(survey)\n",
    "importlib.reload(persona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = persona.PersonaEncoder()\n",
    "\n",
    "path = 'data/01_chats/'\n",
    "JsonChatsFB = {\n",
    "    \"x\": [path + 'x'],\n",
    "    \"y\": [path + 'y'],\n",
    "    \"z\": [path + 'z']\n",
    "}\n",
    "\n",
    "################ ROUGH PREP ####################\n",
    "for name, texts in JsonChatsFB.items():\n",
    "    et.parse_fb_messages(texts, name)\n",
    "\n",
    "#filter\n",
    "et.filter_chats_empty()\n",
    "et.filter_chats_regex(utils.BLACKLIST_CHAT_REGEX_FILTERS)\n",
    "\n",
    "#Pseudonomize\n",
    "for nameid, chat in et.chats.items():\n",
    "    for msg in chat:  \n",
    "        msg.sender = \"Persona\" if msg.sender == \"Elias Salvador Smidt Torjani\"  else \"Friend\"\n",
    "################ ROUGH PREP ####################\n",
    "\n",
    "for name in JsonChatsFB.keys():\n",
    "    et.select_chat_full(name)\n",
    "\n",
    "##subset/mask data to handle too old chats or dominating friend[s]##\n",
    "# select_chat_limited_by_tokens(\"elias\", 6000)\n",
    "# if single ^ friend, or if more below.\n",
    "# for nameid, chat in et.chats.items():\n",
    "#     et.chats[nameid] = chat[int(len(chat)/3 * 2):]\n",
    "\n",
    "token_counts = et.count_all_selected_chat_tokens() # for statistics\n",
    "print(f\"Combined tokens: {sum(token_counts.values())}\")\n",
    "\n",
    "#Optional checkpoint save\n",
    "# 01=et.output()\n",
    "# bu.quickTXT(01, filename=f\"data/checkpoints/01_{bu.get_timestamp()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = \"nomic-embed-text\" # gte-Qwen2-7B-instruct-Q5_K_M-GGUF-8k\n",
    "#TODO Handle cases where messages are too far time apart, \n",
    "#TODO Avoid having mesages from multiple people in one chunk.\n",
    "\n",
    "# Chunking parameters\n",
    "chunkSize = 40     # N of msgs per chunk: 10-90?\n",
    "overlapSize = 10   # N of overlapping msgs between consecutive chunks: 5-50?\n",
    "\n",
    "## generate multiple different chunk versions (e.g., for grid search) ##\n",
    "# chunkSizes = [75]\n",
    "# chunkSize = chunkSizes[0]\n",
    "# overlapSizes = [3]\n",
    "# overlapSize = overlapSizes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for storing chunks â€“ and embeddings later\n",
    "chunks = []\n",
    "chunkTokenCounts = []\n",
    "\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "\n",
    "    # Create overlapping chunks of messages\n",
    "    for i in range(0, num_messages - chunkSize + 1, chunkSize - overlapSize):\n",
    "        chunk = messages[i:i + chunkSize]  # Extract chunk of messages\n",
    "        chunkText = \"\\n\".join(str(msg) for msg in chunk)  # Concatenate messages into a single string\n",
    "        chunks.append(chunkText)  # Append chunk to list of chunks\n",
    "        chunkTokenCounts.append(utils.count_tokens(chunkText))\n",
    "        #TODO This is where we should handle chunks more intelligently ^.\n",
    "\n",
    "avgChunkTokenCount = sum(chunkTokenCounts) / len(chunkTokenCounts)\n",
    "embeddings = []\n",
    "progress, chunksLen = 0, len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunkText in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunksLen}\", end=\"\")\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunkText)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "\n",
    "#optional extra info\n",
    "# total_messages = sum(len(chat) for chat in et.selectedChats.values())\n",
    "# chunks_count = len(chunks)\n",
    "# avg_chunk_char_len = np.mean([len(chunk) for chunk in chunks])\n",
    "\n",
    "# print(\n",
    "#     f\"Chunk count: {chunks_count}\",\n",
    "#     # f\"Average chunk character length: {round( avg_chunk_char_len)}\",\n",
    "#     f\"Rough estimate of tokens per chunk: {round(avg_chunk_char_len / 4)} (4 characters per token)\",\n",
    "#     f\"Messagees in input count: {total_messages}\",\n",
    "#     f\"Messages in chunks count: {stat_total_msgs_in_chunks}\",\n",
    "#     f\"Chunk \\ Input ratio: {round(stat_total_msgs_in_chunks / total_messages,2)} (OVERLAP_SIZE={OVERLAP_SIZE})\",\n",
    "#     f\"Chunk Python type: {type(chunks[0])}\",\n",
    "#     sep=\"\\n\"\n",
    "# ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Survey** ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surv = survey.KanoSurvey()\n",
    "surv = survey.PersonalitySurvey()\n",
    "#The Five Factors of personality are:\n",
    "# Openness - How open a person is to new ideas and experiences\n",
    "# Conscientiousness - How goal-directed, persistent, and organized a person is\n",
    "# Extraversion - How much a person is energized by the outside world\n",
    "# Agreeableness - How much a person puts others' interests and needs ahead of their own\n",
    "# Neuroticism - How sensitive a person is to stress and negative emotional triggers\n",
    "\n",
    "# surv = survey.buildFairnessPrompts()\n",
    "# surv = survey.DictatorGameSurvey()\n",
    "surv.questions[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO, there are many versions rn.\n",
    "\n",
    "chunks = []\n",
    "chunk_token_counts = []\n",
    "for chat in et.selectedChats.values():\n",
    "    messages = list(chat)  # Convert chat iterator to list for easier slicing\n",
    "    num_messages = len(messages)\n",
    "    for i in range(0, num_messages - chunkSize + 1, chunkSize - overlapSize):\n",
    "        chunk = messages[i:i + chunkSize]  # Extract chunk of messages\n",
    "        chunk_text = \"\\\\n\".join(str(msg) for msg in chunk)  # Concatenate msgs into a single string\n",
    "        chunks.append(chunk_text)  # Append chunk to list of chunks\n",
    "        chunk_token_counts.append(utils.count_tokens(chunk_text))  # Append token count of the chunk\n",
    "\n",
    "avg_chunk_token_count = sum(chunk_token_counts) / len(chunk_token_counts)\n",
    "embeddings = []\n",
    "progress, chunks_len = 0, len(chunks)\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "EMBEDDING_NAMEID = f\"03_{bu.get_timestamp()}\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": chunkSize,\n",
    "    \"OVERLAP_SIZE\": overlapSize,\n",
    "    \"chunks_count\": len(chunks),\n",
    "    \"modules_chat\": token_counts,\n",
    "    \"overlap_size\": overlapSize,\n",
    "}# bu.quickTXT(01, filename=f\"data/checkpoints/01_{bu.get_timestamp()}\")\n",
    "\n",
    "bu.quickJSON(AUTO_INFO, f\"data/03_embeddings/{EMBEDDING_NAMEID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/03_embeddings/{EMBEDDING_NAMEID}_embeddings.json\")\n",
    "\n",
    "\n",
    "if isinstance(surv, survey.KanoSurvey):\n",
    "    RETRIEVAL_PROMPT = \"video game features\"\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    SURVEY_TYPE = \"KanoSurvey\",\n",
    "    SURVEY = \"video game preferences\"\n",
    "    METHOD = \"Kano survey\"\n",
    "    WHICH_SURVEY = \"kano\"\n",
    "    PROMPT_LENGTH = 40\n",
    "elif isinstance(surv, survey.PersonalitySurvey):\n",
    "    RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "    DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "    SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "    SURVEY = \"personality traits\"\n",
    "    METHOD = \"OCEAN test\"\n",
    "    WHICH_SURVEY = \"pers\"\n",
    "    PROMPT_LENGTH = 50\n",
    "\n",
    "CHUNKS_COUNT_IN_CTX = 10 # Number of nearby chunks to put in context window\n",
    "\n",
    "########### Serialization ###########\n",
    "# EMBEDDING_ID = f\"{CHUNK_SIZE}-{OVERLAP_SIZE}\"\n",
    "VERSION_ID = f\"8k_{WHICH_SURVEY}\" # pers/kano_{ctx tokens}k\n",
    "CHECKPOINT = f\"{EMBEDDING_ID}-{CHUNKS_COUNT_IN_CTX}-{VERSION_ID}\"\n",
    "AUTO_INFO = {\n",
    "    \"CHUNKS_COUNT_IN_CTX\": CHUNKS_COUNT_IN_CTX,\n",
    "    \"EMBEDDING_ID\": EMBEDDING_ID,\n",
    "    \"VERSION_ID\": VERSION_ID,\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": CHUNK_SIZE,\n",
    "    \"OVERLAP_SIZE\": OVERLAP_SIZE,\n",
    "    \"chunks_count\": chunks_count,\n",
    "    \"total_messages\": total_messages,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "    \"SURVEY and method\": f\"{SURVEY} and {METHOD}\",\n",
    "    \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "    \"DYNAMIC_RETRIEVAL_PROMPTS\": DYNAMIC_RETRIEVAL_PROMPTS,\n",
    "}\n",
    "########### Serialization ###########\n",
    "\n",
    "\n",
    "#/\n",
    "\n",
    "\n",
    "\n",
    "########### Serialization ###########\n",
    "EMBEDDING_ID = f\"{chunkSize}-{overlapSize}\"\n",
    "AUTO_INFO = {\n",
    "    \"model\": EMBED_MODEL,\n",
    "    \"CHUNK_SIZE\": chunkSize,\n",
    "    \"OVERLAP_SIZE\": overlapSize,\n",
    "    \"chunks_count\": chunksCount,\n",
    "    \"total_messages\": totalMsgs,\n",
    "    \"stat_total_msgs_in_chunks\": stat_total_msgs_in_chunks,\n",
    "    \"modules_chat\": token_counts,\n",
    "}\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = []\n",
    "progress, chunks_len = 0, len(chunks) # for progress bar\n",
    "for chunk_text in chunks:\n",
    "    progress += 1\n",
    "    print(f\"\\rChunk {progress}/{chunks_len}\", end=\"\")\n",
    "    embedding = ollama.embeddings(model=EMBED_MODEL, prompt=chunk_text)[\"embedding\"]\n",
    "    embeddings.append(embedding)\n",
    "####################################################\n",
    "# token counts in all similar chunks\n",
    "# tokens_in_chunks = 0\n",
    "# for chunk in chunks_most_similar:\n",
    "#     tokens_in_chunks += utils.count_tokens(chunk)\n",
    "# print(f\"Tokens in chunks: {tokens_in_chunks}\")\n",
    "####################################################\n",
    "bu.if_dir_not_exist_make(\"data/03_embeddings\")\n",
    "bu.quickJSON(AUTO_INFO, f\"data/03_embeddings/{EMBEDDING_ID}_info.json\")\n",
    "bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/03_embeddings/{EMBEDDING_ID}_embeddings.json\")\n",
    "\n",
    "\n",
    "\n",
    "#/\n",
    "\n",
    "\n",
    "\n",
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "SUBJECT = \"eli\"\n",
    "RETRIAVAL_METHODS = [\"static\"]  #, \"hybrid\", \"dynamic\", \n",
    "NUM_RUNS = 3\n",
    "MODEL = \"llama3-70b\"            # \"llama3-8b\", \"mixtral-8x22b\"\n",
    "max_tokens = [0, 4000, 7500]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"llama3-70b\"\n",
    "# MODEL = \"llama3-8b\"\n",
    "MODEL = \"mixtral-8x22b\"\n",
    "RETRIAVAL_METHODS = [\"base\"]#, \"hybrid\"]\n",
    "SUBJECT = \"base\"\n",
    "PROMPT_METHOD =\"IMPERSONATE\"\n",
    "NUM_RUNS = 3\n",
    "\n",
    "CHUNK_SIZES = [75]\n",
    "chunk_size = CHUNK_SIZES[0]\n",
    "OVERLAP_SIZES = [3]\n",
    "overlap_size = OVERLAP_SIZES[0]\n",
    "\n",
    "survs = [survey.KanoSurvey(), survey.PersonalitySurvey()]\n",
    "for surv in survs:\n",
    "    if isinstance(surv, survey.KanoSurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 40\n",
    "        SURVEY_TYPE = \"KanoSurvey\",\n",
    "        WHICH_SURVEY = \"kano\"\n",
    "        RETRIEVAL_PROMPT = \"video game features\"\n",
    "        SURVEY = \"video game preferences\"\n",
    "        METHOD = \"a Kano survey\"\n",
    "    elif isinstance(surv, survey.PersonalitySurvey):\n",
    "        DYNAMIC_RETRIEVAL_PROMPTS = list(surv.questions)\n",
    "        PROMPT_COUNT = 50\n",
    "        SURVEY_TYPE = \"PersonalitySurvey\",\n",
    "        WHICH_SURVEY = \"pers\"\n",
    "        RETRIEVAL_PROMPT = \"openess conciousness extrovert aggreableness neuroticism\"\n",
    "        SURVEY = \"personality traits\"\n",
    "        METHOD = \"an OCEAN test\"\n",
    "    for RETRIAVAL_METHOD in RETRIAVAL_METHODS:\n",
    "        if RETRIAVAL_METHOD == \"base\":\n",
    "            final_prompts = []\n",
    "            prompt_template = \"\"\"\n",
    "for question in surv.questions:\n",
    "    p = [\n",
    "        systemMsg(\n",
    "            \"You are participating in a survey. You will be presented with a series of questions about your {SURVEY}.\",\n",
    "            f\"You must choose answer to the question below with one of the five options: {', '.join(surv.POSSIBLE_ANSWERS)}. The answer must only contain the chosen option. \"\n",
    "        ),\n",
    "        assistantMsg('Understood. I will answer the question below with one of the given options.'),\n",
    "        userMsg(\n",
    "            question,\n",
    "            \"Your choice: \"\n",
    "        )]\n",
    "    final_prompts.append(p)\n",
    "            \"\"\"\n",
    "        else: print(\"not ???\")\n",
    "        exec(prompt_template)\n",
    "        SIM_ID = f\"{SUBJECT}-{WHICH_SURVEY}-{str(max_chunks_count).zfill(2)}_{MODEL}_V7\"\n",
    "        bu.quickJSON(final_prompts, f\"data/5_monster_prep/{SIM_ID}_prompts.json\")\n",
    "        for num_run in range(NUM_RUNS):\n",
    "            instructions = {\n",
    "                \"prompt_file\": f\"batch/prompts/{SIM_ID}_prompts.json\",\n",
    "                \"survey_type\": f\"{SURVEY_TYPE[0]}\",\n",
    "                \"isLocal\": True,\n",
    "                \"LIMIT\": None\n",
    "            }\n",
    "            settings = {\n",
    "                \"model\": MODEL,\n",
    "                \"timeout\": 300}\n",
    "            AUTO_INFO = {\n",
    "                \"CHUNK_SIZE\": chunk_size,\n",
    "                \"OVERLAP_SIZE\": overlap_size,\n",
    "                \"CTX_limit\": max_token,\n",
    "                \"EMBED_MODEL\": EMBED_MODEL,\n",
    "                \"prompt method\": PROMPT_METHOD,\n",
    "                \"retrieval method\": RETRIAVAL_METHOD,\n",
    "                \"RETRIEVAL_PROMPT\": RETRIEVAL_PROMPT,\n",
    "                \"prompt_count\": PROMPT_COUNT,\n",
    "                \"survey\": WHICH_SURVEY,\n",
    "                \"SUBJECT\": SUBJECT,\n",
    "                \"prompt_template\": prompt_template,\n",
    "                \"CHUNKS_COUNT_IN_CTX\": max_chunks_count,\n",
    "                **utils.describe_prompts(final_prompts)\n",
    "            }\n",
    "            bu.quickJSON({\"instructions\": instructions, \"settings\": settings, \"info\": AUTO_INFO}, f\"data/5_monster_prep/batch/{SIM_ID}_{num_run}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)\n",
    "\n",
    "max_tokens = 8000\n",
    "cur_tc = 0 # current token count\n",
    "selected_chunks = []\n",
    "for chunk in chunks_most_similar:\n",
    "    tk_in_chunk = utils.count_tokens(chunk)\n",
    "    if cur_tc + tk_in_chunk >= max_tokens:\n",
    "        break\n",
    "    cur_tc += tk_in_chunk\n",
    "    selected_chunks.append(chunk)\n",
    "print(f\"Tokens in chunks: {cur_tc}\")\n",
    "len(selected_chunks)\n",
    "\n",
    "\n",
    "#/\n",
    "\n",
    "\n",
    "\n",
    "# Display results\n",
    "# bu.if_dir_not_exist_make(\"data/4_chunks\")\n",
    "# bu.quickJSON(AUTO_INFO, f\"data/4_chunks/{CHECKPOINT}-dynamic_info.json\")\n",
    "# bu.quickJSON({\"chunks\": chunks, \"embeddings\": embeddings}, f\"data/4_chunks/{CHECKPOINT}-dynamic_embeddings.json\")\n",
    "############################################ VANITY BELOW ########################################\n",
    "tokens_in_chunks = 0\n",
    "for chunks_most_similar in dynamic_chunks_most_similar:\n",
    "    for chunk in chunks_most_similar:\n",
    "        tokens_in_chunks += utils.count_tokens(chunk)\n",
    "\n",
    "del chunks_most_similar_embeddings # free memory\n",
    "print(f\"Tokens in average chunk group: {tokens_in_chunks/len(dynamic_chunks_most_similar)}\")\n",
    "# bu.quickJSON(dynamic_chunks_most_similar, filename=f\"data/4_chunks/{CHECKPOINT}-dynamic_chunks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set the directory where the JSON files are located\n",
    "directory = 'batch/done/monster_7b'  # Replace with the actual directory path if needed\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file has a .json extension\n",
    "    if filename.endswith('.json'):\n",
    "        # Open the JSON file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Modify the \"model\" value\n",
    "        if \"settings\" in data and \"model\" in data[\"settings\"]:\n",
    "            data[\"settings\"][\"model\"] = \"llama3\"\n",
    "\n",
    "        # Write the modified data back to the file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_most_similar_embeddings = utils.find_most_similar(prompt_embedding, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 8000\n",
    "cur_tc = 0 # current token count\n",
    "selected_chunks = []\n",
    "for chunk in chunks_most_similar:\n",
    "    tk_in_chunk = utils.count_tokens(chunk)\n",
    "    if cur_tc + tk_in_chunk >= max_tokens:\n",
    "        break\n",
    "    cur_tc += tk_in_chunk\n",
    "    selected_chunks.append(chunk)\n",
    "print(f\"Tokens in chunks: {cur_tc}\")\n",
    "len(selected_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
